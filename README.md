# ðŸ¸ FableFrog

**Textâ€‘toâ€‘Speech Storytelling, from prompt to performance â€” in minutes.**

> Hackathon prototype built and shipped in **36 hours** using **OpenAI function calling** + **ElevenLabs** with **20+ emotion controls**, presented via a **Gradio** frontâ€‘end.

---

## Table of Contents

* [Highlights](#highlights)
* [Demo](#demo)
* [How It Works](#how-it-works)
* [Features](#features)
* [Tech Stack](#tech-stack)
* [Quickstart](#quickstart)

  * [Prerequisites](#prerequisites)
  * [Environment Variables](#environment-variables)
  * [Installation](#installation)
  * [Run Locally](#run-locally)
* [Usage](#usage)

  * [Gradio UI](#gradio-ui)
  * [CLI (optional)](#cli-optional)
* [Emotion Controls](#emotion-controls)
* [Project Structure](#project-structure)
* [Benchmarks & Metrics](#benchmarks--metrics)
* [Roadmap](#roadmap)
* [Contributing](#contributing)
* [License](#license)
* [Acknowledgements](#acknowledgements)
* [Maintainers](#maintainers)

---

## Highlights

* ðŸš€ **Built and shipped in 36 hours** using OpenAI API (function calling) + ElevenLabs.
* ðŸŽ­ **20+ emotion parameters** mapped to narration prosody for expressive TTS.
* ðŸ§° **Parsed raw SDK docs & GitHub repos** to integrate LLM outputs â€” **50% faster** integration time.
* ðŸ–¥ï¸ **Gradio frontâ€‘end** served **200+ users** during the hackathon; **+70% engagement** vs. baseline demo.
* ðŸ§ª Fully reproducible: oneâ€‘command startup, .envâ€‘first config.

---

## Demo

* **Live Demo:** *coming soon*
* **Video/GIF:** `assets/demo.gif`
* **Screenshots:** `assets/screenshot_ui.png`

> Tip: Add a quick screen capture showing prompt â†’ scene breakdown â†’ audio playback.

---

## How It Works

```
[User Prompt]
     â†“
[OpenAI (function calling)] â”€â”€â–º returns NarrationPlan JSON (scenes, lines, emotions)
     â†“
[Emotion Mapper] â”€â”€â–º normalizes 20+ controls â†’ ElevenLabs style/prosody
     â†“
[ElevenLabs TTS] â”€â”€â–º generates perâ€‘scene clips (streamed)
     â†“
[Gradio UI] â”€â”€â–º preview, stitch, download
```

**NarrationPlan (LLM function):**

```json
{
  "title": "The Moonlit Marsh",
  "scenes": [
    {
      "id": 1,
      "speaker": "narrator",
      "emotion": { "tone": "wonder", "energy": 0.6, "warmth": 0.7, "pace": 0.95 },
      "text": "In the hush of the marsh, a chorus began..."
    }
  ]
}
```

---

## Features

* **Prompt â†’ Structured Story:** LLM returns a scene list with emotion tags.
* **Expressive TTS:** 20+ normalized emotion knobs (tone, energy, pace, pitch, breathiness, suspense, humor, etc.).
* **Perâ€‘Scene Rendering:** Generate, preview, reâ€‘render scenes independently.
* **Fast Iteration:** Cached prompts and voice presets for rapid tweaks.
* **Gradio Web App:** Oneâ€‘page UI to author, play, and export.

---

## Tech Stack

**Languages & Frameworks:** Python â€¢ OpenAI API â€¢ ElevenLabs â€¢ OS â€¢ Gradio

Optional libs: `pydantic`, `python-dotenv`, `uvicorn`, `fastapi` (if exposing REST), `soundfile`/`pydub` for stitching.

---

## Quickstart

### Prerequisites

* Python **3.10+**
* API keys for **OpenAI** and **ElevenLabs**

### Environment Variables

Create a `.env` file in the repo root:

```
OPENAI_API_KEY=sk-...
ELEVENLABS_API_KEY=eleven-...
ELEVENLABS_VOICE_ID=<default_voice_id>
# Optional
GRADIO_SERVER_NAME=0.0.0.0
GRADIO_SERVER_PORT=7860
```

### Installation

```bash
# 1) Clone
git clone https://github.com/<you>/FableFrog.git && cd FableFrog

# 2) Create & activate venv (recommended)
python -m venv .venv && source .venv/bin/activate  # Windows: .venv\\Scripts\\activate

# 3) Install deps
pip install -r requirements.txt
```

**`requirements.txt` (minimal):**

```
openai>=1.0.0
python-dotenv
gradio>=4.0.0
elevenlabs>=1.0.0
pydantic>=2.0.0
pydub
```

### Run Locally

```bash
python app.py
# or
gradio app.py
```

Open [http://localhost:7860](http://localhost:7860).

---

## Usage

### Gradio UI

1. Enter a **story prompt** (e.g., â€œbedtime tale about a curious frogâ€).
2. Click **Generate Plan** â†’ review scene list & emotions.
3. Click **Synthesize** to render TTS per scene.
4. **Refine** emotions/voice and reâ€‘render selected scenes.
5. **Export** stitched audio (`.mp3`/`.wav`).

### CLI (optional)

```bash
python -m fablefrog.cli \
  --prompt "A cozy autumn story" \
  --voice $ELEVENLABS_VOICE_ID \
  --out out/story.wav
```

---

## Emotion Controls

FableFrog exposes humanâ€‘readable knobs and maps them to ElevenLabs style/prosody under the hood.

**Available controls (nonâ€‘exhaustive):**

* `tone` (neutral, wonder, suspense, humor, gentle, solemn)
* `energy` (0â€“1)
* `pace` (0.5â€“1.5)
* `pitch` (-6 to +6 semitones)
* `warmth`, `clarity`, `breathiness`, `tension`, `urgency`, `emphasis`, `curiosity`, `awe`, `joy`, `sadness`, `anger`, `fear`, `surprise`, `calm`, `whisper` (bool)

**Example:**

```json
{
  "tone": "suspense",
  "energy": 0.55,
  "pace": 0.9,
  "pitch": -1,
  "breathiness": 0.3,
  "emphasis": 0.6
}
```

> Note: These are normalized controls; internal mapping may differ per ElevenLabs voice capability.

---

## Project Structure

```
FableFrog/
â”œâ”€ app.py                 # Gradio entrypoint
â”œâ”€ fablefrog/
â”‚  â”œâ”€ llm.py              # OpenAI client + function schema
â”‚  â”œâ”€ tts.py              # ElevenLabs client + emotion mapping
â”‚  â”œâ”€ pipeline.py         # prompt â†’ plan â†’ audio orchestration
â”‚  â”œâ”€ ui.py               # Gradio components & state
â”‚  â””â”€ utils.py            # I/O, caching, stitching
â”œâ”€ assets/                # demo.gif, screenshots
â”œâ”€ requirements.txt
â”œâ”€ .env.example
â””â”€ README.md
```

---

## Benchmarks & Metrics

* **200+ users** served during hackathon session.
* **+70% user engagement** vs. baseline (timeâ€‘onâ€‘page & reâ€‘render interactions).
* **50% reduction** in integration time by parsing raw SDK docs & GitHub repos programmatically.

> Reproduce by running `scripts/collect_metrics.py` (optional).

---

## Roadmap

* [ ] Multiâ€‘voice casting (narrator + characters)
* [ ] SRT/SSML export with emotion tags
* [ ] Cloud deployment template (Spaces/Render/Fly)
* [ ] Live streaming playback per token
* [ ] Inâ€‘UI audio editing (trim, crossfade)

---

## Contributing

PRs welcome! Please open an issue to discuss substantial changes.

* Run `ruff`/`black` before committing.
* Add tests for emotion mapping edge cases.

---

## License

MIT â€” see `LICENSE`.

---

## Acknowledgements

* **OpenAI** for function calling that structures story plans.
* **ElevenLabs** for expressive TTS.
* **Gradio** for rapid UI prototyping.

---

## Maintainers

* **Keshav Jindal** â€” GitHub: [bananatruck](https://github.com/bananatruck)

> Questions? Open an issue with the label `question`.
